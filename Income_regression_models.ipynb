{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**HANDS-ON SESSION-II**\n",
        "\n",
        "**PROFESSOR: IRINA HASHMI**\n",
        "\n",
        "**REGRESSION MODELS**"
      ],
      "metadata": {
        "id": "8tDyhH-S-r7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required libraries"
      ],
      "metadata": {
        "id": "KQz88EEF_Akc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "KX1wmfzb5sSO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the data using pandas library\n",
        "\n",
        "Delete the empty rows\n",
        "\n",
        "set 'Capital-gain' as the target variable - This is the money earned when an asset is sold at a higher price than its original purchase price. For example, if someone buys stock for 1,000 and later sells it for 1,500, the capital gain is $500."
      ],
      "metadata": {
        "id": "rjWxCR-j_Dtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "income_reg=pd.read_csv('/content/income.csv')\n",
        "income_reg.dropna(inplace=True)\n",
        "target = 'capital-gain'"
      ],
      "metadata": {
        "id": "PwQuKQSg5w-R"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode the categorical columns and perform standardization on the numeric columns. Standardization is an import step for regression models as we need to keep all numerical columns in one single scale."
      ],
      "metadata": {
        "id": "uRagXBF0_u93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define categorical and numerical columns\n",
        "categorical_cols = ['workclass', 'education', 'marital status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
        "numerical_cols = ['age', 'education-num', 'capital-loss', 'hours-per-week']\n",
        "\n",
        "# One-hot encode categorical features\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
        "encoded_data = encoder.fit_transform(income_reg[categorical_cols])\n",
        "\n",
        "# Create a DataFrame from encoded data\n",
        "encoded_columns = encoder.get_feature_names_out(categorical_cols)\n",
        "df_encoded = pd.DataFrame(encoded_data, columns=encoded_columns, index=income_reg.index)\n",
        "\n",
        "# Combine numerical data and encoded categorical data\n",
        "X = pd.concat([income_reg[numerical_cols], df_encoded], axis=1)\n",
        "y = income_reg[target]\n",
        "\n",
        "# Scale the features for better performance of regression models\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "xWRNuBiB6H0w"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into training and test sets. 70% of the original data as training data and 30% of the original data as the test set."
      ],
      "metadata": {
        "id": "8uzUpeZfAlOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "TFbIp9oK6Zd6"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the LinearRegression object and apply it to the data.\n",
        "\n",
        "Regression models in general and evaluated using Mean Square Error and R square.\n",
        "\n",
        "But, for this model the R square is 0.04 which means the model is not a good fit for the data."
      ],
      "metadata": {
        "id": "XncHhZ4HA5GU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the Linear Regression model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred_linear = linear_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Linear Regression model\n",
        "print(\"Linear Regression Mean Squared Error:\", mean_squared_error(y_test, y_pred_linear))\n",
        "print(\"Linear Regression R^2 Score:\", r2_score(y_test, y_pred_linear))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do6CQdV66xHB",
        "outputId": "5b630ae4-a15b-4bf1-fd31-82b161e4cb23"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression Mean Squared Error: 59572016.428556494\n",
            "Linear Regression R^2 Score: 0.04982291214451218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees for regression problems are built by splitting the data into smaller and smaller subsets to predict a continuous target value.\n",
        "\n",
        "Random Forest Regression is a machine learning method that predicts a continuous value by averaging the results of many decision trees."
      ],
      "metadata": {
        "id": "QNy7JnAyBjIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initially the R square for the model is in negetives which suggests that the model is performing worse.\n",
        "\n",
        "In the next steps we perform regularization or apply different regularization terms"
      ],
      "metadata": {
        "id": "BkTl-qCQCGCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Random Forest Regressor\n",
        "print(\"Random Forest Regressor Mean Squared Error:\", mean_squared_error(y_test, y_pred_rf))\n",
        "print(\"Random Forest Regressor R^2 Score:\", r2_score(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNn6TuI78-IP",
        "outputId": "cfcb6397-f068-4e7a-c4f6-4d64ff45fc48"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Regressor Mean Squared Error: 69449198.89273985\n",
            "Random Forest Regressor R^2 Score: -0.10771871616834439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next steps we perform regularization or apply different regularization terms which define the model more precisely which can improve the model performance.\n",
        "\n",
        "And the model performs better, but not as expected."
      ],
      "metadata": {
        "id": "qvFNZdMeCmA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a RandomForestRegressor with regularization parameters\n",
        "regressor = RandomForestRegressor(\n",
        "    n_estimators=100,          # Number of trees\n",
        "    max_depth=10,              # Limit the depth of each tree\n",
        "    min_samples_split=5,       # Minimum samples required to split an internal node\n",
        "    min_samples_leaf=4,        # Minimum samples required to be at a leaf node\n",
        "    max_features='sqrt',       # Use the square root of the total features at each split\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mean_mse:.2f}\")\n",
        "print(f\"R^2 Score: {r2_score(y_test, y_pred):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImHxbzzqnF0a",
        "outputId": "fb0b43aa-b2a3-4215-b46e-7d648caa0ee9"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 50849905.59\n",
            "R^2 Score: 0.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We give a try for cross validation for the regressor, we can see that the performance is same as applying regularization parameters."
      ],
      "metadata": {
        "id": "c6vEOE0KC9Im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X and y are your features and target variable\n",
        "# Define the model with regularization parameters\n",
        "regressor = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=4,\n",
        "    max_features='sqrt',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Set up k-fold cross-validation (5 folds)\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and calculate MSE for each fold\n",
        "scores = cross_val_score(\n",
        "    regressor, X, y, cv=kf, scoring=make_scorer(mean_squared_error)\n",
        ")\n",
        "\n",
        "# Calculate the mean and standard deviation of the MSE scores\n",
        "mean_mse = np.mean(scores)\n",
        "std_mse = np.std(scores)\n",
        "\n",
        "print(f\"Mean MSE from cross-validation: {mean_mse:.2f}\")\n",
        "print(f\"Standard Deviation of MSE: {std_mse:.2f}\")\n",
        "print(f\"R^2 Score: {r2_score(y_test, y_pred):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prK18DDiniM8",
        "outputId": "d1a5c97f-9fd9-4462-8ad5-3e6b7408cf65"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean MSE from cross-validation: 50849905.59\n",
            "Standard Deviation of MSE: 7408346.24\n",
            "R^2 Score: 0.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting Regressor is a machine learning algorithm that builds a strong model by sequentially adding small decision trees, each correcting the errors of the previous ones. It combines these trees to make accurate predictions for continuous values.\n",
        "\n",
        "\n",
        "The model performs a bit better than the Random Forest Regressor."
      ],
      "metadata": {
        "id": "85m0zoeYDOia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the gradient boosting model with different parameters\n",
        "gbm_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred_gbm = gbm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Gradient Boosting Regressor\n",
        "print(\"Gradient Boosting Regressor Mean Squared Error:\", mean_squared_error(y_test, y_pred_gbm))\n",
        "print(\"Gradient Boosting Regressor R^2 Score:\", r2_score(y_test, y_pred_gbm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFT8TUJS98Rd",
        "outputId": "85e1233c-1b44-40c9-fb8c-05ccbed1ec87"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor Mean Squared Error: 58788759.604564026\n",
            "Gradient Boosting Regressor R^2 Score: 0.06231590353010075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OTHER REGRESSION MODELS**:\n",
        "\n",
        "1. Ridge Regression (L2 Regularization):\n",
        "Ridge regression adds a penalty to the sum of the squared coefficients in the linear model, which shrinks the coefficients but keeps all features.\n",
        "This helps prevent overfitting by ensuring that the model doesn’t rely too heavily on any single feature.\n",
        "2. Lasso Regression (L1 Regularization):\n",
        "Lasso regression adds a penalty to the sum of the absolute values of the coefficients, which can shrink some coefficients to zero, effectively selecting features.\n",
        "It’s useful for simplifying the model by keeping only the most important features.\n",
        "3. Elastic Net (Combination of L1 and L2):\n",
        "Elastic Net combines both L1 and L2 penalties, balancing between shrinkage (like Ridge) and feature selection (like Lasso).\n",
        "It’s ideal when you want to retain some features but also simplify the model without relying too heavily on either type of regularization."
      ],
      "metadata": {
        "id": "IyHeng3SEYz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L2 Regularization: Ridge Regression\n",
        "ridge = Ridge(alpha=1.0)  # alpha controls the regularization strength; higher means more regularization\n",
        "ridge.fit(X_train, y_train)\n",
        "ridge_predictions = ridge.predict(X_test)\n",
        "print(\"Ridge MSE:\", mean_squared_error(y_test, ridge_predictions))\n",
        "print(\"Ridge R^2 Score:\", r2_score(y_test, ridge_predictions))\n",
        "\n",
        "# L1 Regularization: Lasso Regression\n",
        "lasso = Lasso(alpha=0.1)  # alpha is the regularization parameter\n",
        "lasso.fit(X_train, y_train)\n",
        "lasso_predictions = lasso.predict(X_test)\n",
        "print(\"Lasso MSE:\", mean_squared_error(y_test, lasso_predictions))\n",
        "print(\"Lasso R^2 Score:\", r2_score(y_test, lasso_predictions))\n",
        "\n",
        "# L1 + L2 Regularization: Elastic Net\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio balances between L1 and L2 (0 = pure L2, 1 = pure L1)\n",
        "elastic_net.fit(X_train, y_train)\n",
        "elastic_net_predictions = elastic_net.predict(X_test)\n",
        "print(\"Elastic Net MSE:\", mean_squared_error(y_test, elastic_net_predictions))\n",
        "print(\"Elastic Net R^2 Score:\", r2_score(y_test, elastic_net_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NK-f5bqEXLJ",
        "outputId": "62e52b39-f90d-44ab-e82b-b3136c28639a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge MSE: 59572408.75693983\n",
            "Ridge R^2 Score: 0.04981665448423622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+10, tolerance: 1.163e+08\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso MSE: 59572227.93484804\n",
            "Lasso R^2 Score: 0.049819538606998726\n",
            "Elastic Net MSE: 59576903.49001714\n",
            "Elastic Net R^2 Score: 0.04974496323317357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSION**:\n",
        "<ol>\n",
        "Linear Regression                           - 0.04  \n",
        "\n",
        "\n",
        "Random Forest Regressor                     - -0.17\n",
        "\n",
        "\n",
        "Random Forest Regressor with regularization - 0.05\n",
        "\n",
        "\n",
        "Gradient Boosting Regressor                 - 0.06\n",
        "\n",
        "\n",
        "Ridge Regression                            - 0.04\n",
        "\n",
        "\n",
        "Lasso Regression                            - 0.04\n",
        "\n",
        "\n",
        "Elastic Regression                          - 0.04\n",
        "</ol>\n",
        "\n",
        "Again in Regression model Gradient Boosting Regressor is the best model among others."
      ],
      "metadata": {
        "id": "6WEhWNLCDrhl"
      }
    }
  ]
}