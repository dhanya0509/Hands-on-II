{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**HANDS-ON SESSION-II**\n",
        "\n",
        "**PROFESSOR: IRINA HASHMI**\n",
        "\n",
        "**REGRESSION MODELS**"
      ],
      "metadata": {
        "id": "8tDyhH-S-r7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required libraries"
      ],
      "metadata": {
        "id": "KQz88EEF_Akc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KX1wmfzb5sSO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the data using pandas library\n",
        "\n",
        "Delete the empty rows\n",
        "\n",
        "Here we use age as the target variable. which means we predict the age factor based on the other features in the data"
      ],
      "metadata": {
        "id": "rjWxCR-j_Dtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "income_reg=pd.read_csv('/content/income.csv')\n",
        "income_reg.dropna(inplace=True)\n",
        "target = 'age'"
      ],
      "metadata": {
        "id": "PwQuKQSg5w-R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode the categorical columns and perform standardization on the numeric columns. Standardization is an import step for regression models as we need to keep all numerical columns in one single scale."
      ],
      "metadata": {
        "id": "uRagXBF0_u93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define categorical and numerical columns\n",
        "categorical_cols = ['workclass', 'education', 'marital status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
        "numerical_cols = ['capital-gain', 'education-num', 'capital-loss', 'hours-per-week']\n",
        "\n",
        "# One-hot encode categorical features\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
        "encoded_data = encoder.fit_transform(income_reg[categorical_cols])\n",
        "\n",
        "# Create a DataFrame from encoded data\n",
        "encoded_columns = encoder.get_feature_names_out(categorical_cols)\n",
        "df_encoded = pd.DataFrame(encoded_data, columns=encoded_columns, index=income_reg.index)\n",
        "\n",
        "# Combine numerical data and encoded categorical data\n",
        "X = pd.concat([income_reg[numerical_cols], df_encoded], axis=1)\n",
        "y = income_reg[target]\n",
        "\n",
        "# Scale the features for better performance of regression models\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "xWRNuBiB6H0w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into training and test sets. 70% of the original data as training data and 30% of the original data as the test set."
      ],
      "metadata": {
        "id": "8uzUpeZfAlOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "TFbIp9oK6Zd6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the LinearRegression object and apply it to the data.\n",
        "\n",
        "Regression models in general and evaluated using Mean Square Error and R square.\n",
        "\n",
        "The linear regression model explains about 42% of the variation in the target variable, with an average prediction error magnitude of 107.78. This is a good start but the model has room for improvement in capturing more of the data's variability."
      ],
      "metadata": {
        "id": "XncHhZ4HA5GU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the Linear Regression model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred_linear = linear_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Linear Regression model\n",
        "print(\"Linear Regression Mean Squared Error:\", mean_squared_error(y_test, y_pred_linear))\n",
        "print(\"Linear Regression R^2 Score:\", r2_score(y_test, y_pred_linear))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do6CQdV66xHB",
        "outputId": "836422a2-fadd-40b7-8066-28e6405d640f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression Mean Squared Error: 107.78235744794655\n",
            "Linear Regression R^2 Score: 0.4196852260291071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees for regression problems are built by splitting the data into smaller and smaller subsets to predict a continuous target value.\n",
        "\n",
        "Random Forest Regression is a machine learning method that predicts a continuous value by averaging the results of many decision trees.\n",
        "\n",
        "\n",
        "The Random Forest Regressor has a Mean Squared Error of 113.99, indicating moderate prediction error. The R² score of 0.386 means the model explains about 38.6% of the variation in the target variable, suggesting room for improvement."
      ],
      "metadata": {
        "id": "QNy7JnAyBjIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Random Forest Regressor\n",
        "print(\"Random Forest Regressor Mean Squared Error:\", mean_squared_error(y_test, y_pred_rf))\n",
        "print(\"Random Forest Regressor R^2 Score:\", r2_score(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNn6TuI78-IP",
        "outputId": "ad4ed335-15b8-45fe-bd37-6e5c99d01a74"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Regressor Mean Squared Error: 113.99224450750934\n",
            "Random Forest Regressor R^2 Score: 0.38625035514037787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next steps we perform regularization or apply different regularization terms which define the model more precisely which can improve the model performance.\n",
        "\n",
        "And the model performs better as the MSE reduced, but not as expected."
      ],
      "metadata": {
        "id": "qvFNZdMeCmA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a RandomForestRegressor with regularization parameters\n",
        "regressor = RandomForestRegressor(\n",
        "    n_estimators=100,          # Number of trees\n",
        "    max_depth=10,              # Limit the depth of each tree\n",
        "    min_samples_split=5,       # Minimum samples required to split an internal node\n",
        "    min_samples_leaf=4,        # Minimum samples required to be at a leaf node\n",
        "    max_features='sqrt',       # Use the square root of the total features at each split\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Random Forest Regressor Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Random Forest Regressor R^2 Score:\", r2_score(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImHxbzzqnF0a",
        "outputId": "5818bd15-f782-4a4d-85cb-1f1fd8403505"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Regressor Mean Squared Error: 108.23857277597425\n",
            "Random Forest Regressor R^2 Score: 0.38625035514037787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We give a try for cross validation for the regressor, we can see that the performance is same as applying regularization parameters.\n",
        "\n",
        "Yet the model performs better than the above models"
      ],
      "metadata": {
        "id": "c6vEOE0KC9Im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X and y are your features and target variable\n",
        "# Define the model with regularization parameters\n",
        "regressor = RandomForestRegressor(\n",
        "    n_estimators=300,\n",
        "    max_depth=5,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=4,\n",
        "    max_features='sqrt',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Set up k-fold cross-validation (5 folds)\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and calculate MSE for each fold\n",
        "scores = cross_val_score(\n",
        "    regressor, X, y, cv=kf, scoring=make_scorer(mean_squared_error)\n",
        ")\n",
        "\n",
        "# Calculate the mean and standard deviation of the MSE scores\n",
        "mean_mse = np.mean(scores)\n",
        "std_mse = np.std(scores)\n",
        "\n",
        "print(f\"Mean MSE from cross-validation: {mean_mse:.2f}\")\n",
        "print(f\"R^2 Score: {r2_score(y_test, y_pred):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prK18DDiniM8",
        "outputId": "27dba337-035d-484a-f81f-276d9d321d32"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean MSE from cross-validation: 123.15\n",
            "R^2 Score: 0.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting Regressor is a machine learning algorithm that builds a strong model by sequentially adding small decision trees, each correcting the errors of the previous ones. It combines these trees to make accurate predictions for continuous values.\n",
        "\n",
        "\n",
        "The model performs a bit better than the Random Forest Regressor."
      ],
      "metadata": {
        "id": "85m0zoeYDOia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the gradient boosting model with different parameters\n",
        "gbm_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred_gbm = gbm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Gradient Boosting Regressor\n",
        "print(\"Gradient Boosting Regressor Mean Squared Error:\", mean_squared_error(y_test, y_pred_gbm))\n",
        "print(\"Gradient Boosting Regressor R^2 Score:\", r2_score(y_test, y_pred_gbm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFT8TUJS98Rd",
        "outputId": "e715f0d2-8a90-4525-f31f-e818af81bf2d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor Mean Squared Error: 99.26577276081892\n",
            "Gradient Boosting Regressor R^2 Score: 0.46553966858109264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OTHER REGRESSION MODELS**:\n",
        "\n",
        "1. Ridge Regression (L2 Regularization):\n",
        "Ridge regression adds a penalty to the sum of the squared coefficients in the linear model, which shrinks the coefficients but keeps all features.\n",
        "This helps prevent overfitting by ensuring that the model doesn’t rely too heavily on any single feature.\n",
        "2. Lasso Regression (L1 Regularization):\n",
        "Lasso regression adds a penalty to the sum of the absolute values of the coefficients, which can shrink some coefficients to zero, effectively selecting features.\n",
        "It’s useful for simplifying the model by keeping only the most important features.\n",
        "3. Elastic Net (Combination of L1 and L2):\n",
        "Elastic Net combines both L1 and L2 penalties, balancing between shrinkage (like Ridge) and feature selection (like Lasso).\n",
        "It’s ideal when you want to retain some features but also simplify the model without relying too heavily on either type of regularization."
      ],
      "metadata": {
        "id": "IyHeng3SEYz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L2 Regularization: Ridge Regression\n",
        "ridge = Ridge(alpha=1.0)  # alpha controls the regularization strength; higher means more regularization\n",
        "ridge.fit(X_train, y_train)\n",
        "ridge_predictions = ridge.predict(X_test)\n",
        "print(\"Ridge MSE:\", mean_squared_error(y_test, ridge_predictions))\n",
        "print(\"Ridge R^2 Score:\", r2_score(y_test, ridge_predictions))\n",
        "\n",
        "# L1 Regularization: Lasso Regression\n",
        "lasso = Lasso(alpha=0.1)  # alpha is the regularization parameter\n",
        "lasso.fit(X_train, y_train)\n",
        "lasso_predictions = lasso.predict(X_test)\n",
        "print(\"Lasso MSE:\", mean_squared_error(y_test, lasso_predictions))\n",
        "print(\"Lasso R^2 Score:\", r2_score(y_test, lasso_predictions))\n",
        "\n",
        "# L1 + L2 Regularization: Elastic Net\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio balances between L1 and L2 (0 = pure L2, 1 = pure L1)\n",
        "elastic_net.fit(X_train, y_train)\n",
        "elastic_net_predictions = elastic_net.predict(X_test)\n",
        "print(\"Elastic Net MSE:\", mean_squared_error(y_test, elastic_net_predictions))\n",
        "print(\"Elastic Net R^2 Score:\", r2_score(y_test, elastic_net_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NK-f5bqEXLJ",
        "outputId": "fdb55dc9-262d-48dc-9274-6f9c40c56e94"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge MSE: 107.78208763508628\n",
            "Ridge R^2 Score: 0.4196866787379987\n",
            "Lasso MSE: 108.43554516528535\n",
            "Lasso R^2 Score: 0.4161683751128419\n",
            "Elastic Net MSE: 108.28502784904337\n",
            "Elastic Net R^2 Score: 0.4169787806784825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSION**:\n",
        "<ol>\n",
        "Linear Regression                           - 0.41\n",
        "\n",
        "\n",
        "Random Forest Regressor                     - 0.38\n",
        "\n",
        "\n",
        "Random Forest Regressor with regularization - 0.38\n",
        "\n",
        "\n",
        "Gradient Boosting Regressor                 - 0.46\n",
        "\n",
        "\n",
        "Ridge Regression                            - 0.41\n",
        "\n",
        "\n",
        "Lasso Regression                            - 0.41\n",
        "\n",
        "\n",
        "Elastic Regression                          - 0.41\n",
        "</ol>\n",
        "\n",
        "Again in Regression model Gradient Boosting Regressor is the best model among others."
      ],
      "metadata": {
        "id": "6WEhWNLCDrhl"
      }
    }
  ]
}